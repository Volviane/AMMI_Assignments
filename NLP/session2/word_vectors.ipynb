{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, sys\n",
    "import numpy as np\n",
    "from heapq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(filename):\n",
    "    fin = io.open(filename, 'r', encoding='utf-8', newline='\\n')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.asarray(list(map(float, tokens[1:])))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ** Word vectors ** \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.060511  ,  0.049607  , -0.20885   ,  0.10349   ,  0.14276   ,\n",
       "        0.13577   , -0.46691   ,  0.15968   , -0.18779   ,  0.36288   ,\n",
       "       -0.10825   ,  0.096172  ,  0.02492   ,  0.11521   , -0.27606   ,\n",
       "       -0.26194   , -0.13782   , -0.21345   , -0.30993   ,  0.18901   ,\n",
       "       -0.68209   ,  0.33919   ,  0.0658    , -0.37599   , -0.25713   ,\n",
       "        0.04412   , -0.23731   , -0.28451   ,  0.13815   ,  0.45995   ,\n",
       "        0.15902   ,  0.21194   , -0.17454   , -0.13933   , -0.24543   ,\n",
       "       -0.010022  , -0.014216  , -0.32172   ,  0.26391   , -0.68158   ,\n",
       "        0.11384   , -0.1007    , -0.37438   , -0.0032968 ,  0.049377  ,\n",
       "       -0.34198   , -0.37895   , -0.075533  ,  0.1981    , -0.20643   ,\n",
       "        0.10688   ,  0.016414  ,  0.17665   ,  0.27321   ,  0.13638   ,\n",
       "        0.15284   ,  0.29849   , -0.071184  , -0.18808   ,  0.14807   ,\n",
       "        0.15638   , -0.04346   , -0.084261  , -0.31188   ,  0.07669   ,\n",
       "        0.16689   ,  0.046978  , -0.0044613 ,  0.15034   ,  0.044469  ,\n",
       "       -0.17874   ,  0.12999   ,  0.33261   , -0.31286   , -0.30324   ,\n",
       "        0.29706   ,  0.6429    ,  0.41145   , -0.20294   ,  0.16044   ,\n",
       "        0.3701    ,  0.01084   , -0.27032   , -0.016884  ,  0.23751   ,\n",
       "        0.35608   ,  0.032294  , -0.17303   ,  0.17443   ,  0.38453   ,\n",
       "        0.27992   , -0.5283    ,  0.17313   , -0.21836   ,  0.081181  ,\n",
       "        0.13729   ,  0.16715   ,  0.30889   , -0.24442   ,  0.13076   ,\n",
       "       -0.068804  ,  0.073057  , -0.1986    ,  0.30648   , -0.1369    ,\n",
       "        0.20642   ,  0.10314   ,  0.52534   ,  0.064588  ,  0.10995   ,\n",
       "        0.41645   , -0.2299    , -0.35345   , -0.0032288 , -0.040348  ,\n",
       "       -0.23598   ,  0.56853   ,  0.56024   , -0.19052   ,  0.58393   ,\n",
       "        0.32387   ,  0.30583   , -0.25508   ,  0.019354  ,  0.11135   ,\n",
       "        0.021737  , -0.11853   ,  0.36747   , -0.057895  ,  0.23546   ,\n",
       "       -0.41887   ,  0.37823   , -0.15761   ,  0.28565   ,  0.095126  ,\n",
       "       -0.14315   ,  0.0023317 ,  0.00089469,  0.090331  ,  0.17275   ,\n",
       "       -0.2438    ,  0.44481   ,  0.18404   ,  0.37714   , -0.20763   ,\n",
       "        0.32928   ,  0.40627   , -0.61913   ,  0.015066  ,  0.057385  ,\n",
       "       -0.052045  , -0.49629   , -0.34996   ,  0.39223   , -0.086797  ,\n",
       "       -0.1082    , -0.022648  ,  0.10939   ,  0.15668   , -0.12614   ,\n",
       "       -0.47992   ,  0.45379   ,  0.33219   ,  0.019447  ,  0.19232   ,\n",
       "       -0.21694   , -0.14249   , -0.23918   ,  0.4192    , -0.055168  ,\n",
       "        0.1447    , -0.60721   , -0.3485    ,  0.23083   ,  0.036926  ,\n",
       "       -0.35305   , -0.092386  , -0.080222  , -0.021972  , -0.1031    ,\n",
       "        0.090546  , -0.051905  ,  0.019614  ,  0.14026   , -0.08431   ,\n",
       "       -0.18615   , -0.091845  ,  0.013875  ,  0.12422   , -0.25188   ,\n",
       "        0.046758  , -0.27269   ,  0.29204   ,  0.54706   , -0.47973   ,\n",
       "       -0.11371   , -0.026664  , -0.071887  , -0.19787   ,  0.10408   ,\n",
       "        0.25837   ,  0.28203   , -0.1551    , -0.086132  ,  0.28638   ,\n",
       "       -0.36912   , -0.019165  ,  0.32328   ,  0.29278   ,  0.11356   ,\n",
       "        0.47558   ,  0.69311   , -0.073515  , -0.16553   , -0.073797  ,\n",
       "        0.12483   ,  0.18801   ,  0.13792   ,  0.085083  ,  0.095687  ,\n",
       "       -0.42781   , -0.13225   , -0.24545   ,  0.030893  , -0.18291   ,\n",
       "        0.025061  , -0.12066   , -0.025049  ,  0.12024   , -0.45174   ,\n",
       "        0.057973  , -0.079457  , -0.27155   , -0.29323   , -0.05956   ,\n",
       "        0.28192   ,  0.15851   ,  0.21663   , -0.0091107 , -0.24441   ,\n",
       "       -0.1219    , -0.39721   , -0.14133   ,  0.095826  , -0.22242   ,\n",
       "       -0.11575   , -0.43438   ,  0.7674    , -0.12505   , -0.526     ,\n",
       "       -0.073029  , -0.024028  ,  0.56502   ,  0.085189  ,  0.057737  ,\n",
       "       -0.13205   , -0.019862  , -0.096355  ,  0.27281   ,  0.37488   ,\n",
       "        0.14874   , -0.35987   ,  0.33971   , -0.054766  ,  0.43555   ,\n",
       "       -0.031802  , -0.25993   ,  0.22579   ,  0.045798  ,  0.085835  ,\n",
       "       -0.03752   , -0.22285   , -0.16877   ,  0.079079  ,  0.023024  ,\n",
       "       -0.26843   ,  0.40446   , -0.1094    , -0.29286   ,  0.48122   ,\n",
       "        0.15426   , -0.020409  , -0.22356   ,  0.23117   , -0.29379   ,\n",
       "        0.12801   ,  0.11845   , -0.031538  , -0.21011   ,  0.0057444 ,\n",
       "        0.65781   , -0.4449    , -0.012941  ,  0.16063   ,  0.082666  ,\n",
       "       -0.4588    , -0.17212   ,  0.2289    , -0.074493  , -0.054251  ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading word vectors\n",
    "\n",
    "print('')\n",
    "print(' ** Word vectors ** ')\n",
    "print('')\n",
    "\n",
    "word_vectors = load_vectors('wiki.en.vec')\n",
    "word_vectors['queen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function computes the cosine similarity between vectors u and v\n",
    "\n",
    "def cosine(u, v):\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    dot_prod = u@v.T\n",
    "    cos_sim = dot_prod/(norm_u * norm_v)\n",
    "    ## FILL CODE\n",
    "    return cos_sim#0.0\n",
    "\n",
    "## This function returns the word corresponding to \n",
    "## nearest neighbor vector of x\n",
    "## The list exclude_words can be used to exclude some\n",
    "## words from the nearest neighbors search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity(apple, apples) = 0.637\n",
      "similarity(apple, banana) = 0.431\n",
      "similarity(apple, tiger) = 0.212\n",
      "similarity(queen, women) = 0.157\n"
     ]
    }
   ],
   "source": [
    "# compute similarity between words\n",
    "\n",
    "print('similarity(apple, apples) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['apples']))\n",
    "print('similarity(apple, banana) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['banana']))\n",
    "print('similarity(apple, tiger) = %.3f' %\n",
    "      cosine(word_vectors['apple'], word_vectors['tiger']))\n",
    "print('similarity(queen, women) = %.3f' %\n",
    "      cosine(word_vectors['queen'], word_vectors['kind']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for nearest neighbors\n",
    "\n",
    "def nearest_neighbor(x, word_vectors, exclude_words=[]):\n",
    "    best_score = -1.0\n",
    "    best_word = ''\n",
    "    for word in word_vectors:\n",
    "        if word not in exclude_words:\n",
    "            sim = cosine(x, word_vectors[word])\n",
    "            if sim > best_score :\n",
    "                best_score = sim\n",
    "                best_word = word\n",
    "        \n",
    "    ## FILL CODE\n",
    "    return best_word\n",
    "\n",
    "## This function return the words corresponding to the\n",
    "## K nearest neighbors of vector x.\n",
    "## You can use the functions heappush and heappop.\n",
    "\n",
    "def knn(x, vectors, k):\n",
    "    heap = []\n",
    "    for word in vectors:\n",
    "        sim = cosine(x, vectors[word])\n",
    "        if len(heap) < k:\n",
    "            heappush(heap , (sim, word))\n",
    "        else :\n",
    "            heappushpop(heap, (sim, word))\n",
    "            \n",
    "    ## FILL CODE\n",
    "\n",
    "    return [heappop(heap) for i in range(len(heap))][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nearest neighbor of cat is: dog\n",
      "\n",
      "cat\n",
      "--------------\n",
      "cat\t1.000\n",
      "cats\t0.732\n",
      "dog\t0.638\n",
      "pet\t0.573\n",
      "rabbit\t0.549\n"
     ]
    }
   ],
   "source": [
    "# looking at nearest neighbors of a word\n",
    "\n",
    "print('The nearest neighbor of cat is: ' +\n",
    "      nearest_neighbor(word_vectors['cat'], word_vectors, exclude_words =['cat', 'cats']))\n",
    "\n",
    "knn_cat = knn(word_vectors['cat'], word_vectors, 5)\n",
    "print('')\n",
    "print('cat')\n",
    "print('--------------')\n",
    "for score, word in knn(word_vectors['cat'], word_vectors, 5):\n",
    "    print(word + '\\t%.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function return the word d, such that a:b and c:d\n",
    "## verifies the same relation\n",
    "\n",
    "def analogy(a, b, c, word_vectors):\n",
    "    a = a.lower()\n",
    "    b = b.lower()\n",
    "    c = c.lower()\n",
    "    \n",
    "    v_a = word_vectors[a]\n",
    "    v_b = word_vectors[b]\n",
    "    v_c = word_vectors[c]\n",
    "    \n",
    "    n_a = v_a/np.linalg.norm(v_a)\n",
    "    n_b = v_b/np.linalg.norm(v_b)\n",
    "    n_c = v_c/np.linalg.norm(v_c)\n",
    "    \n",
    "    \n",
    "#     #norm = np.linalg.norm(v_b - v_a + v_c)\n",
    "#     analogie = ''\n",
    "#     best_score = float('-inf')\n",
    "#     for word in word_vectors:\n",
    "        \n",
    "#         if True in [i in word for i in [a, b,c] ]: #word not in [a,b,c]:\n",
    "#             continue\n",
    "#         n_word = word_vectors[word]/np.linalg.norm(word_vectors[word])\n",
    "#         anal = (n_c + n_b - n_a)@n_word.T\n",
    "#         #anal = ((v_b - v_a + v_c)@word_vectors[word].T)/norm\n",
    "#         if anal > best_score:\n",
    "#             best_score = anal\n",
    "#             analogie = word\n",
    "#     # FILL CODE\n",
    "#     return analogie#''\n",
    "    x = v_b - v_a + v_c\n",
    "    return nearest_neighbor(x, word_vectors, exclude_words=[a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "france - paris + rome = italy\n",
      "king - man + woman = queen\n",
      "rome - italy + france = paris\n"
     ]
    }
   ],
   "source": [
    "# Word analogies\n",
    "\n",
    "print('')\n",
    "print('france - paris + rome = ' + analogy('pAris', 'France', 'rome', word_vectors))\n",
    "print('king - man + woman = ' + analogy('man','king','woman', word_vectors))\n",
    "\n",
    "print('rome - italy + france = ' + analogy('italy', 'rome', 'france', word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "similarity(genius, man) = 0.445\n",
      "similarity(genius, woman) = 0.325\n"
     ]
    }
   ],
   "source": [
    "## A word about biases in word vectors:\n",
    "\n",
    "print('')\n",
    "print('similarity(genius, man) = %.3f' %\n",
    "      cosine(word_vectors['man'], word_vectors['genius']))\n",
    "print('similarity(genius, woman) = %.3f' %\n",
    "      cosine(word_vectors['woman'], word_vectors['genius']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the association strength between:\n",
    "##   - a word w\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "def association_strength(w, A, B, vectors):\n",
    "    strength = 0.0\n",
    "    card_A = len(A)\n",
    "    card_B = len(B)\n",
    "    sum_A = 0.0\n",
    "    sum_B = 0.0\n",
    "    \n",
    "    for a in A:\n",
    "        sum_A += cosine(vectors[w], vectors[a])\n",
    "    sum_A /= card_A\n",
    "    \n",
    "    for b in B:\n",
    "        sum_B += cosine(vectors[w], vectors[b])\n",
    "    sum_B /= card_B\n",
    "    \n",
    "    strength =sum_A - sum_B \n",
    "    ## FILL CODE\n",
    "    return strength\n",
    "\n",
    "## Perform the word embedding association test between:\n",
    "##   - two sets of words X and Y\n",
    "##   - two sets of attributes A and B\n",
    "\n",
    "def weat(X, Y, A, B, vectors):\n",
    "    weat_X = 0.0\n",
    "    weat_Y = 0.0\n",
    "    for x in X:\n",
    "        weat_X += association_strength(x, A, B, vectors)\n",
    "    for y in Y:\n",
    "        weat_Y += association_strength(y, A, B, vectors)\n",
    "    \n",
    "        \n",
    "    score = weat_X - weat_Y#0.0\n",
    "    ## FILL CODE\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embedding association test: 0.847\n"
     ]
    }
   ],
   "source": [
    "## Replicate one of the experiments from:\n",
    "##\n",
    "## Semantics derived automatically from language corpora contain human-like biases\n",
    "## Caliskan, Bryson, Narayanan (2017)\n",
    "\n",
    "career = ['executive', 'management', 'professional', 'corporation', \n",
    "          'salary', 'office', 'business', 'career']\n",
    "family = ['home', 'parents', 'children', 'family',\n",
    "          'cousins', 'marriage', 'wedding', 'relatives']\n",
    "male = ['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n",
    "female = ['amy', 'joan', 'lisa', 'sarah', 'diana', 'kate', 'ann', 'donna']\n",
    "\n",
    "print('')\n",
    "print('Word embedding association test: %.3f' %\n",
    "      weat(career, family, male, female, word_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
