{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dlc_practical_prologue import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The objective of this session is to implement a multi-layer perceptron with one hidden layer from scratch and test on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_test,y_test = load_data(one_hot_labels=True,normalize=True)\n",
    "y_train *=0.9\n",
    "#y_test *=0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = X_train.size(1)\n",
    "hidden_size = 50\n",
    "output_size = y_train.size(1)\n",
    "epsilon = 1e-6\n",
    "\n",
    "w1 = torch.empty(hidden_size, feature_size).normal_(0,epsilon)\n",
    "b1 = torch.empty(hidden_size,1).normal_(0,epsilon)\n",
    "\n",
    "w2 = torch.empty(output_size,hidden_size).normal_(0,epsilon)\n",
    "b2 = torch.empty(output_size,1).normal_(0,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dw1 = torch.zeros(hidden_size, feature_size)\n",
    "dl_db1 = torch.zeros(hidden_size,1)\n",
    "\n",
    "dl_dw2 = torch.zeros(output_size,hidden_size)\n",
    "dl_db2 = torch.zeros(output_size,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def dsigma(x):\n",
    "    return 1 - torch.pow(sigma(x),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(v,t):\n",
    "    return torch.sum(torch.pow((v-t),2))\n",
    "\n",
    "def dloss(v,t):\n",
    "    return 2 * (v-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward and backward  passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(w1,b1,w2,b2,x):\n",
    "    x0 = x\n",
    "    s1 = torch.mv(w1,x0) + b1.flatten()\n",
    "    #print(w1)\n",
    "    x1 = sigma(s1)\n",
    "    s2 = torch.mv(w2, x1) + b2.flatten()\n",
    "    x2 = sigma(s2)\n",
    "    return x0,s1,x1,s2,x2\n",
    "\n",
    "def backward_pass(w1,b1,w2,b2,t,x,s1,x1,s2,x2,dl_dw1,dl_db1,dl_dw2,dl_db2):\n",
    "    x0 = x\n",
    "    dl_dx2 = dloss(x2, t)\n",
    "    dl_ds2 = dsigma(s2) * dl_dx2\n",
    "    dl_dx1 = w2.t().mv(dl_ds2)\n",
    "    dl_ds1 = dsigma(s1) * dl_dx1\n",
    "    \n",
    "    dl_dw2.add_(dl_ds2.view(-1, 1).mm(x1.view(1, -1)))\n",
    "    dl_db2.flatten().add_(dl_ds2)\n",
    "    dl_dw1.add_(dl_ds1.view(-1, 1).mm(x0.view(1, -1)))\n",
    "    dl_db1.flatten().add_(dl_ds1)\n",
    "    \n",
    "    return dl_dw1, dl_db1, dl_dw2, dl_db2  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 acc_train_loss 48.44 train accuracy 99.90 test accuracy 85.10 \n",
      "10 acc_train_loss 43.29 train accuracy 99.90 test accuracy 85.20 \n",
      "20 acc_train_loss 46.94 train accuracy 99.80 test accuracy 85.60 \n",
      "30 acc_train_loss 57.16 train accuracy 99.80 test accuracy 85.70 \n",
      "40 acc_train_loss 45.09 train accuracy 99.90 test accuracy 85.00 \n",
      "50 acc_train_loss 50.35 train accuracy 99.90 test accuracy 83.70 \n",
      "60 acc_train_loss 43.98 train accuracy 99.90 test accuracy 84.30 \n",
      "70 acc_train_loss 58.74 train accuracy 99.90 test accuracy 83.60 \n",
      "80 acc_train_loss 53.50 train accuracy 99.90 test accuracy 83.80 \n",
      "90 acc_train_loss 41.31 train accuracy 99.90 test accuracy 84.30 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in range(100):\n",
    "    pred = []\n",
    "    pred_train = []\n",
    "   #train\n",
    "    acc_loss = 0\n",
    "    nb_train_errors = 0\n",
    "    learning_rate = 0.1/X_train.size(0)\n",
    "\n",
    "    dl_dw1.zero_()\n",
    "    dl_db1.zero_()\n",
    "    dl_dw2.zero_()\n",
    "    dl_db2.zero_()\n",
    "\n",
    "    for n in range(X_train.size(0)):\n",
    "        \n",
    "        x0, s1, x1, s2, x2 = forward_pass(w1, b1, w2, b2, X_train[n])\n",
    "        \n",
    "        acc_loss = acc_loss + loss(x2.T, y_train[n])\n",
    "        \n",
    "\n",
    "        dl_dw1, dl_db1, dl_dw2, dl_db2 = backward_pass(w1, b1, w2, b2,\n",
    "                     y_train[n],\n",
    "                     x0, s1, x1, s2, x2,\n",
    "                     dl_dw1, dl_db1, dl_dw2, dl_db2)\n",
    "    \n",
    "        \n",
    "    w1 = w1 - learning_rate * dl_dw1\n",
    "    b1 = b1 - learning_rate * dl_db1\n",
    "    w2 = w2 - learning_rate * dl_dw2\n",
    "    b2 = b2 - learning_rate * dl_db2\n",
    "    \n",
    "    for i in range(X_train.size(0)):\n",
    "        _, _, _, _, x2 = forward_pass(w1, b1, w2, b2, X_train[i])\n",
    "        pred_train.append(x2.max(0)[1].item())\n",
    "\n",
    "   # Test error\n",
    "    nb_test_errors = 0\n",
    "\n",
    "    for n in range(X_test.size(0)):\n",
    "        \n",
    "        _, _, _, _, x2 = forward_pass(w1, b1, w2, b2, X_test[n])\n",
    "        pred.append(x2.max(0)[1].item())\n",
    "    #print(torch.sum(torch.tensor(pred) == torch.argmax(y_test, 1)).item()/1000)\n",
    "        #if y_test[n, pred] < 0.5: nb_test_errors = nb_test_errors + 1\n",
    "    if k%10 ==0:\n",
    "        print('{:d} acc_train_loss {:.02f} train accuracy {:.02f} test accuracy {:.02f} '.format(k, \n",
    "                                                acc_loss,\n",
    "                                                (torch.sum(torch.argmax(y_train, 1)==torch.tensor(pred_train)).item()/ y_train.size(0))*100,\n",
    "                                                (torch.sum(torch.argmax(y_test, 1)==torch.tensor(pred)).item()/ y_test.size(0))*100))\n",
    "    \n",
    "#     print('{:d} acc_train_loss {:.02f} acc_train_error {:.02f}% test_error {:.02f}%'\n",
    "#          .format(k, acc_loss,\n",
    "#                  (100 * nb_train_errors) / X_train.size(0),\n",
    "#                  (100 * nb_test_errors) / X_test.size(0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
